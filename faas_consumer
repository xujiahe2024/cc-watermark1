import base64
import json
from urllib.parse import unquote
from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip, concatenate_videoclips
import os
from google.cloud import firestore
from google.cloud import storage
import time
import concurrent.futures

import tempfile

output_dir = os.path.abspath('./output')


tmpdir = tempfile.gettempdir()


project_id = "watermarking-424614"
topic_id = "image-watermark"


Storage = storage.Client()
database = firestore.Client()

#tmpdir = os.path.abspath('./output')

bucketname = "ccmarkbucket"


#parallel downloading of chunks
def download_chunk(chunk, chunk_path_web):
    global bucketname
    global Storage
    chunk_blob = storage.bucket(bucketname).blob(chunk_path_web)
    chunk_blob.download_to_filename(chunk)

def process_chunk(job_id, video_url, watermark_path, start, end, current_chunk, total_chunks):
        global Storage
        global database
        #print(f"Processing chunk {current_chunk} of {total_chunks} for job {job_id}")
        start_time = time.time()
        job_ref = database.collection('job').document(job_id)
        
        video_path = f'{tmpdir}/{job_id}_video_{current_chunk}.webm'
        
        if not os.path.exists(tmpdir):
            os.makedirs(tmpdir)
        
        right_video_url = f'videos/{job_id}_{current_chunk}.webm'
        
        if not os.path.exists(video_path):
            blob = Storage.bucket('ccmarkbucket').blob(right_video_url)
            blob.download_to_filename(video_path)
            
            
        watermark_path = os.path.join(tmpdir, f'{job_id}_watermark.png')
        if not os.path.exists(watermark_path):
            blob = Storage.bucket('ccmarkbucket').blob(f'watermarks/{job_id}.png')
            blob.download_to_filename(watermark_path)
            
        #print(f"Downloaded video for job {job_id}")
        download_time = time.time()
        
        video = VideoFileClip(video_path)

        #video.duration = 10
        #video = VideoFileClip(video_path).subclip(start, end)
        watermark = ImageClip(watermark_path).set_duration(video.duration)
        watermark = watermark.resize(height=50).margin(right=8, bottom=8, opacity=0).set_position(("right", "bottom"))

        processed = CompositeVideoClip([video, watermark])
        
        process_time = time.time()
        
        os.remove(video_path)
        os.remove(watermark_path)
        
        chunk_path = f'{tmpdir}/{job_id}_final_chunk{current_chunk}.webm'
        #tmpfilePath = f'{tmpdir}/{job_id}_final_chunk_{current_chunk}_temp_audiofile_path'
        processed.write_videofile(chunk_path, temp_audiofile_path = tmpdir, logger = None)
        
        print(f"Processed chunk {current_chunk} of {total_chunks} for job {job_id}")

        bucket = Storage.bucket('ccmarkbucket')
        blob = bucket.blob(f'final/{job_id}_final_chunk{current_chunk}.webm')
        blob.upload_from_filename(chunk_path)
        
        #delete chunk file
        os.remove(chunk_path)
        
        process_time = time.time()

        #print(f"Uploaded chunk {current_chunk} of {total_chunks} for job {job_id}")
        job_data = job_ref.get().to_dict()
        #print(f"Job data1: {job_data}")
        
        job_ref.update({'completed_chunks': firestore.Increment(1)})
        job_ref.set({f'task_{current_chunk}':{
            'download_time': download_time - start_time,
            'process_time': process_time - download_time,
            'upload_time': time.time() - process_time
            }}, merge=True)

        job_data = job_ref.get().to_dict()
        #print(f"Job data2: {job_data}")
        video.close()
        processed.close()
        watermark.close()
        
        if job_data['completed_chunks'] >= job_data['total_chunks']:
            print(f"All chunks processed for job {job_id}")
            merge_chunks(job_id)
            job_ref.update({'status': 'completed', 'progress': 100})


"""
def merge_chunks(job_id):
    database = firestore.Client()
    job_ref = database.collection('job').document(job_id)
    job_data = job_ref.get().to_dict()
    #print(f"Job data3: {job_data}")
    chunks_path = [f'{tmpdir}/{job_id}_final_chunk{current_chunk}.webm' for current_chunk in range(job_data['total_chunks'])]
    #print(f"chunks_path: {chunks_path}")
    clips = [VideoFileClip(chunk) for chunk in chunks_path]
    final_clip = concatenate_videoclips(clips)
    #tmpfilePath = f'{tmpdir}/{job_id}_final_temp_audiofile_path'
    final_clip.write_videofile(f'{tmpdir}/final_{job_id}.mp4', temp_audiofile_path = tmpdir, logger = None)
    final_result_path = f'{tmpdir}/final_{job_id}.mp4'

    bucket = Storage.bucket('ccmarkbucket')
    final_blob = bucket.blob(f'{tmpdir}/{job_id}_final.mp4')
    final_blob.upload_from_filename(final_result_path)
    
    os.remove(final_result_path)
    
    print(f"Uploaded final result for job {job_id}")

"""


def merge_chunks(job_id):
    global database
    global storage
    job_ref = database.collection('job').document(job_id)
    job_data = job_ref.get().to_dict()
    #print(f"Job data3: {job_data}")
    chunks_path_web = [f'final/{job_id}_final_chunk{current_chunk}.webm' for current_chunk in range(job_data['total_chunks'])]
    chunks_path = [f'{output_dir}/{job_id}_final_chunk{current_chunk}.webm' for current_chunk in range(job_data['total_chunks'])]
    #print(f"chunks_path: {chunks_path}")
    
    """
    for i, chunk in enumerate(chunks_path):
        chunk_blob = storage.bucket(bucketname).blob(chunks_path_web[i])
        chunk_blob.download_to_filename(chunk)
    """
    
    with concurrent.futures.ThreadPoolExecutor() as executor:
        executor.map(download_chunk, chunks_path, chunks_path_web)
    
        
    clips = [VideoFileClip(chunk) for chunk in chunks_path]
    final_clip = concatenate_videoclips(clips)
    #tmpfilePath = f'{tmpdir}/{job_id}_final_temp_audiofile_path'
    #final_clip.upload_video(f'{output_dir}/final_{job_id}.mp4', codec = "libvpx", logger = None)
    final_clip.write_videofile(f'{output_dir}/final_{job_id}.mp4', temp_audiofile_path = output_dir, logger = None)
    final_result_path = f'{output_dir}/final_{job_id}.mp4'

    bucket = storage.bucket('ccmarkbucket')
    final_blob = bucket.blob(f'{output_dir}/{job_id}_final.mp4')
    final_blob.upload_from_filename(final_result_path)
    
    #os.remove(final_result_path)
    #for chunk in chunks_path:
    #    os.remove(chunk)
    
    job_ref.update({'status': 'completed', 'progress': 100, 'resulturl': final_blob.public_url})
    
    #print(f"Uploaded final result for job {job_id}")




def hello_pubsub(event, context):
    """Triggered from a message on a Cloud Pub/Sub topic.
    Args:
         event (dict): Event payload.
         context (google.cloud.functions.Context): Metadata for the event.
    """
    try:        
                pubsub_message = base64.b64decode(event['data']).decode('utf-8')
                #print(f"Received message : {pubsub_message}")
                pubsub_message = json.loads(pubsub_message)

                process_chunk(pubsub_message['job_id'], "", pubsub_message['watermark_path'], pubsub_message['start'], pubsub_message['end'], pubsub_message['chunk_num'], pubsub_message['total_chunks'])
    except Exception as e:
                print(f"An error occurred while processing message: {e}")